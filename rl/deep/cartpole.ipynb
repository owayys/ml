{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66ef473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in /home/owais/anaconda3/lib/python3.13/site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/owais/anaconda3/lib/python3.13/site-packages (from gymnasium[classic-control]) (2.3.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/owais/anaconda3/lib/python3.13/site-packages (from gymnasium[classic-control]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/owais/anaconda3/lib/python3.13/site-packages (from gymnasium[classic-control]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/owais/anaconda3/lib/python3.13/site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\n",
      "  Downloading pygame-2.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gymnasium[classic-control]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8253666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "646cd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78c78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.memory = deque(maxlen=10_000)\n",
    "\n",
    "        self.gamma         = 0.90   # discount rate\n",
    "        self.epsilon       = 1.0    # exploration rate\n",
    "        self.epsilon_min   = 0.01\n",
    "        self.epsilon_decay = 0.98\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model     = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def memorize(self, memory: tuple):\n",
    "        self.memory.append(memory)\n",
    "\n",
    "    def get_action(self, state) -> int:\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # exploit\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            state_t  = self._to_tensor(state)           # shape: (1, state_size)\n",
    "            q_values = self.model(state_t)               # shape: (1, action_size)\n",
    "        return int(q_values.argmax(dim=1).item())\n",
    "\n",
    "    def train(self, batch_size: int = 32):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states      = np.vstack([m[0] for m in minibatch])          # (B, state_size)\n",
    "        actions     = np.array( [m[1] for m in minibatch])          # (B,)\n",
    "        rewards     = np.array( [m[2] for m in minibatch], dtype=np.float32)\n",
    "        next_states = np.vstack([m[3] for m in minibatch])          # (B, state_size)\n",
    "        dones       = np.array( [m[4] for m in minibatch], dtype=np.float32)\n",
    "\n",
    "        states_t      = self._to_tensor(states)\n",
    "        next_states_t = self._to_tensor(next_states)\n",
    "        rewards_t     = self._to_tensor(rewards)\n",
    "        dones_t       = self._to_tensor(dones)\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        q_current = self.model(states_t)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_next = self.model(next_states_t)                      # (B, action_size)\n",
    "            q_next_max = q_next.max(dim=1).values                   # (B,)\n",
    "\n",
    "        q_target = q_current.clone()\n",
    "        batch_indices = torch.arange(batch_size, device=self.device)\n",
    "        actions_t = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # Bellman equation\n",
    "        q_target[batch_indices, actions_t] = (\n",
    "            rewards_t + self.gamma * q_next_max * (1.0 - dones_t)\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(q_current, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # decay exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a09042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owais/anaconda3/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[episode 0/300] total reward: 10, epsilon: 1.00\n",
      "[episode 1/300] total reward: 14, epsilon: 1.00\n",
      "[episode 2/300] total reward: 43, epsilon: 1.00\n",
      "[episode 3/300] total reward: 13, epsilon: 1.00\n",
      "[episode 4/300] total reward: 14, epsilon: 1.00\n",
      "[episode 5/300] total reward: 13, epsilon: 1.00\n",
      "[episode 6/300] total reward: 28, epsilon: 1.00\n",
      "[episode 7/300] total reward: 10, epsilon: 0.98\n",
      "[episode 8/300] total reward: 21, epsilon: 0.96\n",
      "[episode 9/300] total reward: 19, epsilon: 0.94\n",
      "[episode 10/300] total reward: 15, epsilon: 0.92\n",
      "[episode 11/300] total reward: 17, epsilon: 0.90\n",
      "[episode 12/300] total reward: 18, epsilon: 0.89\n",
      "[episode 13/300] total reward: 9, epsilon: 0.87\n",
      "[episode 14/300] total reward: 34, epsilon: 0.85\n",
      "[episode 15/300] total reward: 24, epsilon: 0.83\n",
      "[episode 16/300] total reward: 11, epsilon: 0.82\n",
      "[episode 17/300] total reward: 40, epsilon: 0.80\n",
      "[episode 18/300] total reward: 22, epsilon: 0.78\n",
      "[episode 19/300] total reward: 31, epsilon: 0.77\n",
      "[episode 20/300] total reward: 11, epsilon: 0.75\n",
      "[episode 21/300] total reward: 31, epsilon: 0.74\n",
      "[episode 22/300] total reward: 48, epsilon: 0.72\n",
      "[episode 23/300] total reward: 16, epsilon: 0.71\n",
      "[episode 24/300] total reward: 16, epsilon: 0.70\n",
      "[episode 25/300] total reward: 16, epsilon: 0.68\n",
      "[episode 26/300] total reward: 13, epsilon: 0.67\n",
      "[episode 27/300] total reward: 12, epsilon: 0.65\n",
      "[episode 28/300] total reward: 12, epsilon: 0.64\n",
      "[episode 29/300] total reward: 15, epsilon: 0.63\n",
      "[episode 30/300] total reward: 8, epsilon: 0.62\n",
      "[episode 31/300] total reward: 10, epsilon: 0.60\n",
      "[episode 32/300] total reward: 19, epsilon: 0.59\n",
      "[episode 33/300] total reward: 14, epsilon: 0.58\n",
      "[episode 34/300] total reward: 10, epsilon: 0.57\n",
      "[episode 35/300] total reward: 13, epsilon: 0.56\n",
      "[episode 36/300] total reward: 11, epsilon: 0.55\n",
      "[episode 37/300] total reward: 10, epsilon: 0.53\n",
      "[episode 38/300] total reward: 15, epsilon: 0.52\n",
      "[episode 39/300] total reward: 11, epsilon: 0.51\n",
      "[episode 40/300] total reward: 9, epsilon: 0.50\n",
      "[episode 41/300] total reward: 13, epsilon: 0.49\n",
      "[episode 42/300] total reward: 40, epsilon: 0.48\n",
      "[episode 43/300] total reward: 9, epsilon: 0.47\n",
      "[episode 44/300] total reward: 8, epsilon: 0.46\n",
      "[episode 45/300] total reward: 13, epsilon: 0.45\n",
      "[episode 46/300] total reward: 12, epsilon: 0.45\n",
      "[episode 47/300] total reward: 7, epsilon: 0.44\n",
      "[episode 48/300] total reward: 12, epsilon: 0.43\n",
      "[episode 49/300] total reward: 10, epsilon: 0.42\n",
      "[episode 50/300] total reward: 12, epsilon: 0.41\n",
      "[episode 51/300] total reward: 8, epsilon: 0.40\n",
      "[episode 52/300] total reward: 12, epsilon: 0.39\n",
      "[episode 53/300] total reward: 22, epsilon: 0.39\n",
      "[episode 54/300] total reward: 13, epsilon: 0.38\n",
      "[episode 55/300] total reward: 14, epsilon: 0.37\n",
      "[episode 56/300] total reward: 9, epsilon: 0.36\n",
      "[episode 57/300] total reward: 9, epsilon: 0.36\n",
      "[episode 58/300] total reward: 7, epsilon: 0.35\n",
      "[episode 59/300] total reward: 11, epsilon: 0.34\n",
      "[episode 60/300] total reward: 13, epsilon: 0.34\n",
      "[episode 61/300] total reward: 18, epsilon: 0.33\n",
      "[episode 62/300] total reward: 9, epsilon: 0.32\n",
      "[episode 63/300] total reward: 13, epsilon: 0.32\n",
      "[episode 64/300] total reward: 26, epsilon: 0.31\n",
      "[episode 65/300] total reward: 9, epsilon: 0.30\n",
      "[episode 66/300] total reward: 8, epsilon: 0.30\n",
      "[episode 67/300] total reward: 8, epsilon: 0.29\n",
      "[episode 68/300] total reward: 9, epsilon: 0.29\n",
      "[episode 69/300] total reward: 14, epsilon: 0.28\n",
      "[episode 70/300] total reward: 12, epsilon: 0.27\n",
      "[episode 71/300] total reward: 8, epsilon: 0.27\n",
      "[episode 72/300] total reward: 8, epsilon: 0.26\n",
      "[episode 73/300] total reward: 7, epsilon: 0.26\n",
      "[episode 74/300] total reward: 7, epsilon: 0.25\n",
      "[episode 75/300] total reward: 10, epsilon: 0.25\n",
      "[episode 76/300] total reward: 9, epsilon: 0.24\n",
      "[episode 77/300] total reward: 9, epsilon: 0.24\n",
      "[episode 78/300] total reward: 12, epsilon: 0.23\n",
      "[episode 79/300] total reward: 8, epsilon: 0.23\n",
      "[episode 80/300] total reward: 9, epsilon: 0.22\n",
      "[episode 81/300] total reward: 9, epsilon: 0.22\n",
      "[episode 82/300] total reward: 9, epsilon: 0.22\n",
      "[episode 83/300] total reward: 9, epsilon: 0.21\n",
      "[episode 84/300] total reward: 10, epsilon: 0.21\n",
      "[episode 85/300] total reward: 8, epsilon: 0.20\n",
      "[episode 86/300] total reward: 8, epsilon: 0.20\n",
      "[episode 87/300] total reward: 12, epsilon: 0.19\n",
      "[episode 88/300] total reward: 10, epsilon: 0.19\n",
      "[episode 89/300] total reward: 10, epsilon: 0.19\n",
      "[episode 90/300] total reward: 9, epsilon: 0.18\n",
      "[episode 91/300] total reward: 9, epsilon: 0.18\n",
      "[episode 92/300] total reward: 9, epsilon: 0.18\n",
      "[episode 93/300] total reward: 9, epsilon: 0.17\n",
      "[episode 94/300] total reward: 10, epsilon: 0.17\n",
      "[episode 95/300] total reward: 8, epsilon: 0.17\n",
      "[episode 96/300] total reward: 9, epsilon: 0.16\n",
      "[episode 97/300] total reward: 9, epsilon: 0.16\n",
      "[episode 98/300] total reward: 9, epsilon: 0.16\n",
      "[episode 99/300] total reward: 8, epsilon: 0.15\n",
      "[episode 100/300] total reward: 9, epsilon: 0.15\n",
      "[episode 101/300] total reward: 8, epsilon: 0.15\n",
      "[episode 102/300] total reward: 11, epsilon: 0.14\n",
      "[episode 103/300] total reward: 8, epsilon: 0.14\n",
      "[episode 104/300] total reward: 8, epsilon: 0.14\n",
      "[episode 105/300] total reward: 9, epsilon: 0.14\n",
      "[episode 106/300] total reward: 12, epsilon: 0.13\n",
      "[episode 107/300] total reward: 8, epsilon: 0.13\n",
      "[episode 108/300] total reward: 8, epsilon: 0.13\n",
      "[episode 109/300] total reward: 9, epsilon: 0.12\n",
      "[episode 110/300] total reward: 7, epsilon: 0.12\n",
      "[episode 111/300] total reward: 10, epsilon: 0.12\n",
      "[episode 112/300] total reward: 8, epsilon: 0.12\n",
      "[episode 113/300] total reward: 11, epsilon: 0.12\n",
      "[episode 114/300] total reward: 8, epsilon: 0.11\n",
      "[episode 115/300] total reward: 8, epsilon: 0.11\n",
      "[episode 116/300] total reward: 11, epsilon: 0.11\n",
      "[episode 117/300] total reward: 10, epsilon: 0.11\n",
      "[episode 118/300] total reward: 9, epsilon: 0.10\n",
      "[episode 119/300] total reward: 10, epsilon: 0.10\n",
      "[episode 120/300] total reward: 9, epsilon: 0.10\n",
      "[episode 121/300] total reward: 9, epsilon: 0.10\n",
      "[episode 122/300] total reward: 7, epsilon: 0.10\n",
      "[episode 123/300] total reward: 9, epsilon: 0.09\n",
      "[episode 124/300] total reward: 8, epsilon: 0.09\n",
      "[episode 125/300] total reward: 8, epsilon: 0.09\n",
      "[episode 126/300] total reward: 8, epsilon: 0.09\n",
      "[episode 127/300] total reward: 10, epsilon: 0.09\n",
      "[episode 128/300] total reward: 8, epsilon: 0.09\n",
      "[episode 129/300] total reward: 9, epsilon: 0.08\n",
      "[episode 130/300] total reward: 9, epsilon: 0.08\n",
      "[episode 131/300] total reward: 9, epsilon: 0.08\n",
      "[episode 132/300] total reward: 8, epsilon: 0.08\n",
      "[episode 133/300] total reward: 9, epsilon: 0.08\n",
      "[episode 134/300] total reward: 8, epsilon: 0.08\n",
      "[episode 135/300] total reward: 9, epsilon: 0.07\n",
      "[episode 136/300] total reward: 8, epsilon: 0.07\n",
      "[episode 137/300] total reward: 8, epsilon: 0.07\n",
      "[episode 138/300] total reward: 8, epsilon: 0.07\n",
      "[episode 139/300] total reward: 9, epsilon: 0.07\n",
      "[episode 140/300] total reward: 9, epsilon: 0.07\n",
      "[episode 141/300] total reward: 8, epsilon: 0.07\n",
      "[episode 142/300] total reward: 9, epsilon: 0.06\n",
      "[episode 143/300] total reward: 9, epsilon: 0.06\n",
      "[episode 144/300] total reward: 8, epsilon: 0.06\n",
      "[episode 145/300] total reward: 8, epsilon: 0.06\n",
      "[episode 146/300] total reward: 8, epsilon: 0.06\n",
      "[episode 147/300] total reward: 11, epsilon: 0.06\n",
      "[episode 148/300] total reward: 9, epsilon: 0.06\n",
      "[episode 149/300] total reward: 8, epsilon: 0.06\n",
      "[episode 150/300] total reward: 9, epsilon: 0.05\n",
      "[episode 151/300] total reward: 8, epsilon: 0.05\n",
      "[episode 152/300] total reward: 7, epsilon: 0.05\n",
      "[episode 153/300] total reward: 11, epsilon: 0.05\n",
      "[episode 154/300] total reward: 9, epsilon: 0.05\n",
      "[episode 155/300] total reward: 8, epsilon: 0.05\n",
      "[episode 156/300] total reward: 9, epsilon: 0.05\n",
      "[episode 157/300] total reward: 9, epsilon: 0.05\n",
      "[episode 158/300] total reward: 8, epsilon: 0.05\n",
      "[episode 159/300] total reward: 8, epsilon: 0.05\n",
      "[episode 160/300] total reward: 13, epsilon: 0.04\n",
      "[episode 161/300] total reward: 10, epsilon: 0.04\n",
      "[episode 162/300] total reward: 10, epsilon: 0.04\n",
      "[episode 163/300] total reward: 10, epsilon: 0.04\n",
      "[episode 164/300] total reward: 8, epsilon: 0.04\n",
      "[episode 165/300] total reward: 8, epsilon: 0.04\n",
      "[episode 166/300] total reward: 9, epsilon: 0.04\n",
      "[episode 167/300] total reward: 11, epsilon: 0.04\n",
      "[episode 168/300] total reward: 11, epsilon: 0.04\n",
      "[episode 169/300] total reward: 8, epsilon: 0.04\n",
      "[episode 170/300] total reward: 8, epsilon: 0.04\n",
      "[episode 171/300] total reward: 9, epsilon: 0.04\n",
      "[episode 172/300] total reward: 10, epsilon: 0.03\n",
      "[episode 173/300] total reward: 11, epsilon: 0.03\n",
      "[episode 174/300] total reward: 10, epsilon: 0.03\n",
      "[episode 175/300] total reward: 9, epsilon: 0.03\n",
      "[episode 176/300] total reward: 10, epsilon: 0.03\n",
      "[episode 177/300] total reward: 10, epsilon: 0.03\n",
      "[episode 178/300] total reward: 8, epsilon: 0.03\n",
      "[episode 179/300] total reward: 10, epsilon: 0.03\n",
      "[episode 180/300] total reward: 10, epsilon: 0.03\n",
      "[episode 181/300] total reward: 9, epsilon: 0.03\n",
      "[episode 182/300] total reward: 10, epsilon: 0.03\n",
      "[episode 183/300] total reward: 9, epsilon: 0.03\n",
      "[episode 184/300] total reward: 11, epsilon: 0.03\n",
      "[episode 185/300] total reward: 11, epsilon: 0.03\n",
      "[episode 186/300] total reward: 11, epsilon: 0.03\n",
      "[episode 187/300] total reward: 11, epsilon: 0.03\n",
      "[episode 188/300] total reward: 9, epsilon: 0.03\n",
      "[episode 189/300] total reward: 10, epsilon: 0.02\n",
      "[episode 190/300] total reward: 11, epsilon: 0.02\n",
      "[episode 191/300] total reward: 8, epsilon: 0.02\n",
      "[episode 192/300] total reward: 12, epsilon: 0.02\n",
      "[episode 193/300] total reward: 10, epsilon: 0.02\n",
      "[episode 194/300] total reward: 11, epsilon: 0.02\n",
      "[episode 195/300] total reward: 9, epsilon: 0.02\n",
      "[episode 196/300] total reward: 9, epsilon: 0.02\n",
      "[episode 197/300] total reward: 11, epsilon: 0.02\n",
      "[episode 198/300] total reward: 8, epsilon: 0.02\n",
      "[episode 199/300] total reward: 12, epsilon: 0.02\n",
      "[episode 200/300] total reward: 10, epsilon: 0.02\n",
      "[episode 201/300] total reward: 12, epsilon: 0.02\n",
      "[episode 202/300] total reward: 9, epsilon: 0.02\n",
      "[episode 203/300] total reward: 9, epsilon: 0.02\n",
      "[episode 204/300] total reward: 9, epsilon: 0.02\n",
      "[episode 205/300] total reward: 12, epsilon: 0.02\n",
      "[episode 206/300] total reward: 10, epsilon: 0.02\n",
      "[episode 207/300] total reward: 11, epsilon: 0.02\n",
      "[episode 208/300] total reward: 12, epsilon: 0.02\n",
      "[episode 209/300] total reward: 11, epsilon: 0.02\n",
      "[episode 210/300] total reward: 12, epsilon: 0.02\n",
      "[episode 211/300] total reward: 10, epsilon: 0.02\n",
      "[episode 212/300] total reward: 11, epsilon: 0.02\n",
      "[episode 213/300] total reward: 12, epsilon: 0.02\n",
      "[episode 214/300] total reward: 10, epsilon: 0.01\n",
      "[episode 215/300] total reward: 11, epsilon: 0.01\n",
      "[episode 216/300] total reward: 12, epsilon: 0.01\n",
      "[episode 217/300] total reward: 10, epsilon: 0.01\n",
      "[episode 218/300] total reward: 12, epsilon: 0.01\n",
      "[episode 219/300] total reward: 12, epsilon: 0.01\n",
      "[episode 220/300] total reward: 11, epsilon: 0.01\n",
      "[episode 221/300] total reward: 11, epsilon: 0.01\n",
      "[episode 222/300] total reward: 10, epsilon: 0.01\n",
      "[episode 223/300] total reward: 12, epsilon: 0.01\n",
      "[episode 224/300] total reward: 12, epsilon: 0.01\n",
      "[episode 225/300] total reward: 11, epsilon: 0.01\n",
      "[episode 226/300] total reward: 10, epsilon: 0.01\n",
      "[episode 227/300] total reward: 11, epsilon: 0.01\n",
      "[episode 228/300] total reward: 11, epsilon: 0.01\n",
      "[episode 229/300] total reward: 11, epsilon: 0.01\n",
      "[episode 230/300] total reward: 11, epsilon: 0.01\n",
      "[episode 231/300] total reward: 11, epsilon: 0.01\n",
      "[episode 232/300] total reward: 15, epsilon: 0.01\n",
      "[episode 233/300] total reward: 15, epsilon: 0.01\n",
      "[episode 234/300] total reward: 15, epsilon: 0.01\n",
      "[episode 235/300] total reward: 14, epsilon: 0.01\n",
      "[episode 236/300] total reward: 12, epsilon: 0.01\n",
      "[episode 237/300] total reward: 10, epsilon: 0.01\n",
      "[episode 238/300] total reward: 12, epsilon: 0.01\n",
      "[episode 239/300] total reward: 14, epsilon: 0.01\n",
      "[episode 240/300] total reward: 15, epsilon: 0.01\n",
      "[episode 241/300] total reward: 14, epsilon: 0.01\n",
      "[episode 242/300] total reward: 11, epsilon: 0.01\n",
      "[episode 243/300] total reward: 16, epsilon: 0.01\n",
      "[episode 244/300] total reward: 15, epsilon: 0.01\n",
      "[episode 245/300] total reward: 19, epsilon: 0.01\n",
      "[episode 246/300] total reward: 14, epsilon: 0.01\n",
      "[episode 247/300] total reward: 15, epsilon: 0.01\n",
      "[episode 248/300] total reward: 17, epsilon: 0.01\n",
      "[episode 249/300] total reward: 7, epsilon: 0.01\n",
      "[episode 250/300] total reward: 8, epsilon: 0.01\n",
      "[episode 251/300] total reward: 9, epsilon: 0.01\n",
      "[episode 252/300] total reward: 16, epsilon: 0.01\n",
      "[episode 253/300] total reward: 9, epsilon: 0.01\n",
      "[episode 254/300] total reward: 8, epsilon: 0.01\n",
      "[episode 255/300] total reward: 8, epsilon: 0.01\n",
      "[episode 256/300] total reward: 9, epsilon: 0.01\n",
      "[episode 257/300] total reward: 9, epsilon: 0.01\n",
      "[episode 258/300] total reward: 9, epsilon: 0.01\n",
      "[episode 259/300] total reward: 8, epsilon: 0.01\n",
      "[episode 260/300] total reward: 9, epsilon: 0.01\n",
      "[episode 261/300] total reward: 9, epsilon: 0.01\n",
      "[episode 262/300] total reward: 9, epsilon: 0.01\n",
      "[episode 263/300] total reward: 8, epsilon: 0.01\n",
      "[episode 264/300] total reward: 8, epsilon: 0.01\n",
      "[episode 265/300] total reward: 8, epsilon: 0.01\n",
      "[episode 266/300] total reward: 9, epsilon: 0.01\n",
      "[episode 267/300] total reward: 15, epsilon: 0.01\n",
      "[episode 268/300] total reward: 8, epsilon: 0.01\n",
      "[episode 269/300] total reward: 16, epsilon: 0.01\n",
      "[episode 270/300] total reward: 14, epsilon: 0.01\n",
      "[episode 271/300] total reward: 13, epsilon: 0.01\n",
      "[episode 272/300] total reward: 13, epsilon: 0.01\n",
      "[episode 273/300] total reward: 13, epsilon: 0.01\n",
      "[episode 274/300] total reward: 13, epsilon: 0.01\n",
      "[episode 275/300] total reward: 9, epsilon: 0.01\n",
      "[episode 276/300] total reward: 12, epsilon: 0.01\n",
      "[episode 277/300] total reward: 11, epsilon: 0.01\n",
      "[episode 278/300] total reward: 13, epsilon: 0.01\n",
      "[episode 279/300] total reward: 9, epsilon: 0.01\n",
      "[episode 280/300] total reward: 12, epsilon: 0.01\n",
      "[episode 281/300] total reward: 10, epsilon: 0.01\n",
      "[episode 282/300] total reward: 9, epsilon: 0.01\n",
      "[episode 283/300] total reward: 14, epsilon: 0.01\n",
      "[episode 284/300] total reward: 13, epsilon: 0.01\n",
      "[episode 285/300] total reward: 12, epsilon: 0.01\n",
      "[episode 286/300] total reward: 10, epsilon: 0.01\n",
      "[episode 287/300] total reward: 12, epsilon: 0.01\n",
      "[episode 288/300] total reward: 12, epsilon: 0.01\n",
      "[episode 289/300] total reward: 10, epsilon: 0.01\n",
      "[episode 290/300] total reward: 14, epsilon: 0.01\n",
      "[episode 291/300] total reward: 12, epsilon: 0.01\n",
      "[episode 292/300] total reward: 14, epsilon: 0.01\n",
      "[episode 293/300] total reward: 16, epsilon: 0.01\n",
      "[episode 294/300] total reward: 16, epsilon: 0.01\n",
      "[episode 295/300] total reward: 10, epsilon: 0.01\n",
      "[episode 296/300] total reward: 14, epsilon: 0.01\n",
      "[episode 297/300] total reward: 11, epsilon: 0.01\n",
      "[episode 298/300] total reward: 12, epsilon: 0.01\n",
      "[episode 299/300] total reward: 11, epsilon: 0.01\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_episodes = 300\n",
    "render     = False\n",
    "batch_size = 128\n",
    "\n",
    "env          = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "state_size   = env.observation_space.shape[0]\n",
    "action_size  = env.action_space.n\n",
    "agent        = DQNAgent(state_size, action_size)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for t in range(500):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        state_next, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state_next = np.reshape(state_next, [1, state_size])\n",
    "\n",
    "        agent.memorize((state, action, reward, state_next, done))\n",
    "\n",
    "        if done:\n",
    "            print(\n",
    "                f\"[episode {episode}/{n_episodes}] \"\n",
    "                f\"total reward: {t}, epsilon: {agent.epsilon:.2f}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
